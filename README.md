# Oink Oink â€“ Datathon FME 2025 â€“ Mango

## ğŸ“– DescripciÃ³n

Este repositorio contiene nuestro **pipeline final de predicciÃ³n de demanda para Mango**, desarrollado para el **Datathon FME 2025**.

Objetivo: **predecir la cantidad Ã³ptima de producciÃ³n de prendas para la prÃ³xima temporada** usando:

* Embeddings de imÃ¡genes de productos ğŸ–¼ï¸
* Atributos de las prendas ğŸ‘—
* Historial de ventas y producciÃ³n ğŸ“Š

La versiÃ³n **`8.py`** es la final que nos permitiÃ³ alcanzar **55.57900 de accuracy**, combinando los mejores modelos en un **ensemble ponderado**.

---

## ğŸ“‚ Estructura del Repositorio

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv        # Datos histÃ³ricos de entrenamiento
â”‚   â””â”€â”€ test.csv         # Datos de test para predicciÃ³n
â”œâ”€â”€ notebooks/           # Notebooks exploratorios (opcional)
â”œâ”€â”€ 1.py â€¦ 7.py          # Versiones previas de experimentos
â””â”€â”€ 8.py                 # Pipeline final (ensemble de finalistas)
```

---

## âš™ï¸ Pipeline Final (`8.py`)

### Pasos principales:

1. **ImportaciÃ³n de librerÃ­as**

   * pandas, numpy, sklearn, catboost, etc.

2. **ConfiguraciÃ³n global**

   * Paths, parÃ¡metros PCA, cross-validation, pesos del ensemble

3. **IngenierÃ­a de caracterÃ­sticas**

   * Limpieza y agregaciÃ³n de datos
   * Parsing y PCA de embeddings de imagen
   * Features agregadas por familia, categorÃ­a y atributos
   * NormalizaciÃ³n logarÃ­tmica de features numÃ©ricas

4. **Entrenamiento de modelos finalistas**

   * **Modelo A**: Alpha=0.78, learning_rate=0.01 (mÃ¡s estable)
   * **Modelo B**: Alpha=0.75, learning_rate=0.03 (mÃ¡s agresivo)
   * CatBoost con **K-Fold CV** para seleccionar iteraciones Ã³ptimas

5. **Ensemble ponderado**

   * 60% Modelo A + 40% Modelo B
   * TransformaciÃ³n inversa log1p para obtener predicciones reales

6. **GeneraciÃ³n de submission**

   * Archivo `submission_catboost_V18_EnsembleFinalists.csv` listo para Kaggle/Datathon

---

## ğŸ› ï¸ Requisitos

* Python >= 3.9
* pandas
* numpy
* scikit-learn
* catboost

```bash
pip install pandas numpy scikit-learn catboost
```

---

## ğŸš€ Uso

1. Coloca `train.csv` y `test.csv` en la carpeta `data/`
2. Ejecuta el pipeline final:

```bash
python 8.py
```

3. ObtendrÃ¡s `submission_catboost_V18_EnsembleFinalists.csv` con las predicciones finales.

---

## ğŸ† Logros y Aprendizajes

* Ensemble de modelos CatBoost alcanzÃ³ **55.57900 de accuracy**
* Feature engineering robusto fue mÃ¡s determinante que hiperajustar modelos complejos
* CombinaciÃ³n de embeddings de imagen, atributos categÃ³ricos y datos histÃ³ricos multi-temporada fue clave
* ValidaciÃ³n temporal (TimeSeriesSplit) evitÃ³ fugas de informaciÃ³n y permitiÃ³ modelos generalizables

---

## ğŸ”® PrÃ³ximos pasos

* Entrenar embeddings visuales propios
* Explorar TabNet o LightGBM con tuning automÃ¡tico
* AÃ±adir interpretabilidad al pipeline para entender quÃ© atributos generan mÃ¡s demanda
* Automatizar todo el flujo para producciÃ³n real

---

## ğŸ“Œ CrÃ©ditos

Equipo **Oink Oink** â€“ Estudiantes de Inteligencia Artificial UPC, Datathon FME 2025.
