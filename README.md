# Oink Oink ‚Äì Datathon FME 2025 ‚Äì Mango

## üìñ Descripci√≥n

Este repositorio contiene nuestro **pipeline final de predicci√≥n de demanda para Mango**, desarrollado para el **Datathon FME 2025**.

Objetivo: **predecir la cantidad √≥ptima de producci√≥n de prendas para la pr√≥xima temporada** usando:

* Embeddings de im√°genes de productos üñºÔ∏è
* Atributos de las prendas üëó
* Historial de ventas y producci√≥n üìä

La versi√≥n **`8.py`** es la final que nos permiti√≥ alcanzar **55.57900 de accuracy**, combinando los mejores modelos en un **ensemble ponderado**.

---

## Estructura del Repositorio

```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ train.csv        # Datos hist√≥ricos de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ test.csv         # Datos de test para predicci√≥n 
‚îú‚îÄ‚îÄ 1.py ‚Ä¶ 7.py          # Versiones previas de experimentos
‚îî‚îÄ‚îÄ 8.py                 # Pipeline final (ensemble de finalistas)
```

---

## Pipeline Final (`8.py`)

### Pasos principales:

1. **Importaci√≥n de librer√≠as**

   * pandas, numpy, sklearn, catboost, etc.

2. **Configuraci√≥n global**

   * Paths, par√°metros PCA, cross-validation, pesos del ensemble

3. **Ingenier√≠a de caracter√≠sticas**

   * Limpieza y agregaci√≥n de datos
   * Parsing y PCA de embeddings de imagen
   * Features agregadas por familia, categor√≠a y atributos
   * Normalizaci√≥n logar√≠tmica de features num√©ricas

4. **Entrenamiento de modelos finalistas**

   * **Modelo A**: Alpha=0.78, learning_rate=0.01 (m√°s estable)
   * **Modelo B**: Alpha=0.75, learning_rate=0.03 (m√°s agresivo)
   * CatBoost con **K-Fold CV** para seleccionar iteraciones √≥ptimas

5. **Ensemble ponderado**

   * 60% Modelo A + 40% Modelo B
   * Transformaci√≥n inversa log1p para obtener predicciones reales

6. **Generaci√≥n de submission**

   * Archivo `submission_catboost_V18_EnsembleFinalists.csv` listo para Kaggle/Datathon

---

## Requisitos

* Python >= 3.9
* pandas
* numpy
* scikit-learn
* catboost

```bash
pip install pandas numpy scikit-learn catboost
```

---

## Uso

1. Coloca `train.csv` y `test.csv` en la carpeta `data/`
2. Ejecuta el pipeline final:

```bash
python 8.py
```

3. Obtendr√°s `submission_catboost_V18_EnsembleFinalists.csv` con las predicciones finales.

---

## Logros y Aprendizajes

* Ensemble de modelos CatBoost alcanz√≥ **55.57900 de accuracy**
* Feature engineering robusto fue m√°s determinante que hiperajustar modelos complejos
* Combinaci√≥n de embeddings de imagen, atributos categ√≥ricos y datos hist√≥ricos multi-temporada fue clave
* Validaci√≥n temporal (TimeSeriesSplit) evit√≥ fugas de informaci√≥n y permiti√≥ modelos generalizables

---

## Pr√≥ximos pasos

* Entrenar embeddings visuales propios
* Explorar TabNet o LightGBM con tuning autom√°tico
* A√±adir interpretabilidad al pipeline para entender qu√© atributos generan m√°s demanda
* Automatizar todo el flujo para producci√≥n real

---

## Cr√©ditos

Equipo **Oink Oink** ‚Äì Estudiantes de Inteligencia Artificial UPC, Datathon FME 2025.
